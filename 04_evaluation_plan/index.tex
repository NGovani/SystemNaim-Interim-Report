\chapter{Evaluation Plan}
\label{chap:Eval}

\section{Introduction}
The evaluation of this project will not only be concerned with administrative metrics such as: were deadlines met, how much of the original scope was implemented? There will also be metrics evaluating the performance of the tool.

Of special interest are two metrics, user difficulty and performance. SystemNaim aims to make it easier for users to implement multi-FPGA systems and thus one factor that it's success hinges on is the complexity of the tool to the user. This must be minimised for this endeavour to be at all useful, otherwise the user may as well have used pre-existing tool to implement their solution.

Performance is also important. If there is too much of a performance decrease going to a multi-FPGA system using SystemNaim, then the tool once again becomes impractical for the user. Below is further explanation on 

\section{User Difficulty}
For the time-to-market metric we intend to compare against other available HLS tools as well as pure Verilog. Not only will the time-taken to implement a given system be considered but also the amount of extra research a user must do in order to achieve their desired design. Ideally, this evaluation will be done by asking a sample of individuals to implement a multi-FPGA system using a variety of tools, however, given the current state of things it may be difficult to find participants; thus the proposed back-up is to do a part qualitative part quantitative analysis of how difficult it was for a single person to implement a sample system using multiple methods.

\section{Performance}
On the other hand, the performance metric has a few more complications that we must consider. Firstly, performance is a vague term, and we must define more specific metrics in order to measure this value. There are two main metrics that shall be used in this project: 

\begin{itemize}
    \item \textbf{Latency}: The amount of time it takes for a system to compute it's intended function. Measured either in milliseconds or cycles.
    \item \textbf{Resource Usage}: The amount of resources a system takes up on an FPGA. Measures in number of LUTs, BRAMs and DSPs.
\end{itemize}

These metrics can be further broken down, however for the purpose of this project these will tell us all the information we need\footnote{Throughput may also be seen as important however with loop pipe-lining likely not being implemented it serves little purpose.}.

The second issue is that tools such as Vivado HLS or LegUp have spent years in development and thus comparing the latency of a design produced by a product of their calibre versus something made in less than 6 months would not benefit any evaluation of the specific issue SystemNaim aims to solve. If we were to compare SystemNaim to an industry tool, we wouldn't be able to see the benefit of using multiple FPGAs to increase the performance of a system. Rather, we would merely see how much better optimised the Verilog that the industry tools produce is, over SystemNaim's output Verilog.

Therefore, the methodology we intend to use to measure performance is to compare the latency and resource usage of a single FPGA system created using SystemNaim and a multi-FPGA system created from the same tool. This way we can identify the performance increase/decrease specifically attributed to the tool's method of implementing multi-FPGA systems. 

The conclusion that we hope to draw from this, is that if this method of automating the connection between multiple FPGAs is beneficial, then it is possible to expand this methodology to other tools. So even if the core HLS aspect of SystemNaim is subpar, the process by which it creates multi-FPGA systems adds value to the field of research.



