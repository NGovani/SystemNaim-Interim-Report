\chapter{Background}
\label{chap:Background}

\section{Introduction}

The field of multi-FPGAs has been heavily researched however there are few papers interested in utilising HLS to streamline the process of implementing a multi-FPGA. \cite{564741} and \cite{707888} are examples of papers which do concern themselves with these topics but both were written over twenty years ago.

Therefore, it is of interest to this project to do a review of the most up to date research in fields surrounding and including its scope. This chapter will go through four different topics:

\begin{itemize}
    \item Related works.
    \item HLS tools and their optimisations.
    \item FPGA-to-FPGA communication protocols.
    \item Multi-FPGA Logic Partitioning.
\end{itemize}

Thus, hopefully, giving the reader a better overview of what is possible if this project were to reach its full potential.

\section{Related Work}
\label{sec:related_work}

As previously mentioned, \cite{564741} and \cite{707888} are papers which have already attempted to solve the issues raised by this project. The solution in the former uses the GNU C++ compiler as well as a method of describing the algorithm as a set of difference equations. One point the authors explicitly state is that they utilise C++'s operator overloading feature in order to map software operations to specific hardware modules. Nonetheless, once the circuit has been generated, it is then automatically partitioned using a method based on Fiducia-Mattheyses bi-partitioning heuristic \cite{1585498} with the goal of this stage to maximise logic utilisation in each FPGA as well as minimising IO consumption.

The method by which the FPGAs communicated with each other is not discussed in the paper, all that is mentioned is the method used for partitioning. There is, however, mention of a host CPU, and it may be reasonable to assume that this CPU acts as a master for all FPGAs meaning there is no direct communication between FPGAs.

The latter paper, \cite{707888}, opts to place a psuedo-processor on each FPGA within the topology and then have a top-level controller supply each FPGA with the instructions it is meant to execute. The HLS aspect then merely needs to convert the supplied source code into a list of instruction which is the intended use of standard compilers. The motivation for this approach was to make multi-FPGA systems more accessible to software developers, which is its main target demographic. Alongside this, the paper also presents how it uses a 4 dimensional space, with one dimension being each FPGA system, to optimise the datapath of the system. This then supposedly creates a control path where the FPGAs are used in parallel wherever possible.

\section{HLS Tools \& Optimisations}
\label{sec:bg:hls}

\subsection{Introduction}

Different HLS tools have vastly different implementations \cite{7368920} and many of them, especially commercial tools, do not reveal their methodologies. LegUp\cite{legup} for example, compiles the input source code using the LLVM compiler and then profiles it on a modified MIPS CPU emulator in order identify which parts of the code would benefit from hardware acceleration. It then synthesis those specific parts to dedicated hardware, while the rest of the code is run using a MIPS CPU programmed onto the FPGA.

Vivado HLS, on the other hand, makes no attempt to suggest to the user where acceleration is possible. Rather, it merely gives you tools known as PRAGMAS, found in \cite{vivado-optmisation-manual}, to help the compiler better understand how to convert the C code to hardware. The output is then a set Verilog files the user can implement as they see fit, though it is recommended to use the Vivado Design Suite \cite{vivado-design-suite-manual} as part of the development cycle. As of now, Vivado has not revealed how they go from C to Verilog.

A third contender is Bambu~\cite{6645550}, which leverages the GCC IR in order to optimise the resulting hardware and focuses mainly on memory intensive programs. In short, each implementation has its own intended purpose and thus excels in a specific area, but the fact that there are many implementations indicate that this field has much room from improvement and is actively under development.

\subsection{Input Language}

For the most part, HLS tools tend be based around compiling a subset of C or C++ to Verilog \cite{7368920}, however, there is an argument to be made that functional programming languages have a certain affinity towards hardware development. \cite{bjesse1998lava} presents an early form of the idea, Lava, which takes Haskell as input and produces circuit specifications. The tool was built in the early days of HDLs, and thus was intended to be a competitor rather than a part of the tool chain. More modern approaches such as \cite{hardcaml} are libraries for existing languages and instead produce Verilog or SystemVerilog which can be later synthesised by the appropriate tools. But why bother with straying so far from the norm. In \cite{7331371}, it is suggested that the firm mathematical founding in which functional languages are based can assist in verifying hardware designs as well as offering benefits such as recursive functions and type inference. While the author in \cite{Edwards2019FHWP}, suggests that the functional paradigm can allow for a greater exploitation of parallelism, one of the driving reason to using FPGAs. 

Evidently, the field of research in using functional languages is of interest to quite a few individuals. \cite{7723553} lists a few papers in its introduction whilst also contributing to the field itself, but it has yet to be seen whether Functional HLS will overtake standard C-based HLS anytime soon.

\subsection{Loop Pipelining}

One of the major benefits of running a system on an FPGA is its ability to perform computation in parallel. Loop pipelining is a perfect example of this, where a HLS tool will take a for loop or while loop written in the source code and run segments of it concurrently in order to reach a low initiation interval(II). Page 120 of \cite{vivado-optmisation-manual} and page 11 of \cite{intel-hls-manual} show how the two leading HLS tools, Vivado HLS and Intel High Level Synthesis Compiler, implement pipelining respectively. The II is a measure of how many a new iteration of the loop can start and a lower value, ideally one, means that the section of code containing the loop will be computed faster. 

There are issues, however, with attempting to reach an $II = 1$. \cite{7544750} explores how memory dependencies, two parts of a loop accessing the same memory resources, can cause increases in II due to the HLS compiler not being able to find a way to resolve these dependencies. Instead, it proposes the idea of splitting loops in order to reach an II of 1 on each of these sub-loops and then to pipe the data from one sub-loop to the next. Another approach shown in \cite{7160061} opts to implement parametric hardware to resolve similar dependencies. 

In summary, for simple programs with static loop behaviour the loop pipelining strategies found in commercial tools is very effective and likely the most important optimisation. However, when the behaviour becomes uncertain more effort is required on the user's side to help the compiler reach lower II's.

\subsection{Data Streaming}

In order to process data in parallel on an FPGA the data must first be available to the device or module doing the processing, an obvious point but one that must be considered for any HLS system. In general, there are two approaches to solve the problem of ensuring data availability, the first is atomic transactions while the second is data streaming/dataflow. Say that we have two modules, $A$ and $B$ which are connected such that the output of $A$, which is an array, is the input of $B$. Let us also say that both modules iterate over an array doing some processing, i.e. multiplying the elements by 2 or adding 5, on said array. In an atomic system we would first let $A$ iterate over the entire array and then pass the array to $B$ and allow it to compute its function. 

In this case, even though the modules themselves may be pipelined there isn't much parallelism being exploited on the module level. $B$ is sitting idle for the entire time $A$ is operating and vice-versa. The solution to this is dataflow, which is where every time $A$ completes an iteration it then passes that data to $B$ allowing to start its computation. This results in both modules being utilised at the same time and reduces the overall latency of the system. Section 3.3 of \cite{9264692} explores further, why a user might opt for this method and the considerations they must take if they choose to do so. 

In relation to HLS tools, this optimisation is pivotal to achieving high throughput in any system that operators on large amount of data. Pages 20-24, 95-97 and 126-128 of \cite{vivado-optmisation-manual} show how Vivado HLS allows its users to access such functionality.


\section{FPGA-to-FPGA Communication}
\label{sec:bg:comm}

\subsection{Communication Medium}

One of the major considerations that must be made when deciding the structure of a multi-FPGA system is the communication medium between the FPGAs. Many options are possible, \cite{10.1145/3358192} utilises SFP+ connections whilst \cite{10.1145/3337821.3337846} connects the board using a PCI-E interface. Both examples require ports to be connected to FPGA I/O pins and are usually approaches that are, usually, only used when working development boards. If, one only has access to the pure FPGA itself, unlikely nowadays, then pin implementations are a much more apt choice. \cite{658564} goes into very fine detail about the necessity of a good pin assignment algorithm to reduce unnecessary resource consumption.

It seems that more modern implementations tend to use a standardised communication medium such as Ethernet, PCI-E or Serial, which makes sense given the large overhead usually required when developing a new communication protocol. \cite{7890234} even explores making the communication medium agnostic to the user application, which allows the user logic to access a single interface which then handles to packaging of data for any medium. For example, one FPGA may only have access to an Ethernet port while another only has an RS232 port, but this system would allow these two FPGA's to transfer data between them.

\subsection{Purpose of the System}

While there are many options for communication mediums available to a designer, one must first question the purpose of the system in order to evaluate the efficacy of a communication method. 

A \textbf{PCI-E} communication medium tends to be chosen when there is a single host communicating with multiple client's such as in \cite{10.1145/3337821.3337846} or \cite{6927459}. PCI-E does not allow for clients to interact directly with each other and instead only allows host-client communication. 

In contrast, \textbf{Ethernet} allows the user to identify each device in their topology with a number (MAC Address) and then lets each device send data to every other device in the network (Section 8 in \cite{6847097}) making it more suited to Peer-2-Peer systems. 

Both have their place in the ecosystem with latest PCI-E generations reaching 128 GB/s \cite{pcie-speed} compared to Ethernet reaching at most 1.25 GB/s \cite{ether-speed}, whilst is Ethernet better suited for more complicated topologies with numerous hosts.

\section{Logic Partitioning}
\label{sec:logic_part}

\subsection{Manual vs. Automatic}

Logic partitioning concerns itself with how a designer split's the computation across the multi-FPGA system. Generally speaking, there are two approaches to this a manual split and an automatic split. A manual split is where the designer is given control of the computation processed on every FPGA, however usual implementations following this methodology tend to be a single design replicated on multiple FPGAs. \cite{10.1145/3020078.3021739, 10.1145/3358192, 10.1145/3337821.3337846} all have one FPGA overlay that supports communication either with a host or other peers and is able to slot into a pre-existing network in a modular fashion. Thus, it would seem that any manual splitting is done with scalability in mind and a repeatable design is used to reduce decrease complexity and development time.

The second approach is an automatic split, and it seems that papers regarding this method tend to concern themselves with more general input than a specific use case. Both papers mentioned in \autoref{sec:related_work} and \cite{soton261305} are examples of this approach. Interestingly, \cite{707888} uses a similar practice to what was described of the manual approach by having a single design, that has a more general use-case, overlayed onto many FPGAs. Whereas \cite{564741, soton261305} instead opt to use a partitioning algorithm, which splits a design to fit the requested number of devices. 

\subsection{Control Schema}

Another aspect to logic partitioning, automatic or manual, is the method by which the workload is distributed. Once again, there seem to be two main approaches. One is for a host device to control each FPGA within system, supplying it with data and also telling it when to start execution. This seems to be the case for \cite{707888} and \cite{10.1145/3337821.3337846}.

On the other hand, \cite{10.1145/3020078.3021739, 10.1145/3358192, soton261305} choose to allow each FPGA to perform its own computation and communicate, whenever it needs to, with other devices on the network. This method while having more critical points, is much more scalable than the first approach which results in a constraint being put upon the system by the hosts computational ability.